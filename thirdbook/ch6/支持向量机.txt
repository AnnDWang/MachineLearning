支持向量机：
优点：
泛化错误率低，计算开销不大，结果易解释
缺点：
对参数调节和核函数的选择敏感，原始分类器不加修改仅适用于处理二类问题
适用数据类型：
数值型和标称型


线性可分数据

N维的数据，需要N-1维的超平面作为分类的决策边界

离分隔超平面最近的点到分隔面的距离为间隔。

支持向量就是离分隔超平面最近的那些点。

SVM的一般流程:
（1）收集数据：可以使用任意方法
（2）准备数据：需要数值型数据
（3）分析数据：有助于可视化分隔超平面
（4）训练算法：SVM的大部分时间有源自训练，该过程主要实现两个参数的调优
（5）测试算法：十分简单的计算过程就可以实现
（6）使用算法：几乎所有的分类问题都可以使用SVM，值得一提的是，svm本身是一个二类分类器，对多类问题应用SVM需要对代码做一些修改

SMO表示序列最小优化（Sequential Minimal Optimization）。SMO算法是将大优化问题分解为多个小优化问题来求解的。这些小优化问题往往很容易求解，并且对他们进行顺序求解的结果与将他们作为整体求解的结果是完全一致的
在结果完全相同的同时，SMO算法的求解时间短很多

SMO的算法的工作原理是：每次循环中选择两个alpha进行优化处理，一旦找到一对合适的alpha，那么就增大其中一个的同时减小另一个，这里所谓的合适就是指alpha必须符合一定的条件，条件之一就是两个alpha必须要在间隔边界之外，而第二个条件则是这两个alpha还没有进行过区间化处理或者不在边界上。

同时改变两个alpha的原因是要保证 sum(alpha*label)=0这个约束条件成立，只改变一个alpha可能会导致该约束失效。

SMO函数伪代码：
创建一个alpha响亮并将其初始化为0向量：
当迭代次数小于最大迭代次数时（外循环）
    对数据集中的每个数据向量（内循环）：
        如果该数据向量可以被优化：
           随机选择另外一个数据向量
           同时优化这两个向量
           如果两个向量都不能被优化，推出内循环
    如果所有向量都没有被优化，增加迭代数目，继续下一次循环


