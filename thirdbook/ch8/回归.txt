线性回归：
优点：结果易于理解，计算上不复杂
缺点：对非线性的数据拟合不好
适用数据类型：数值型和标称型数据

回归的目的是预测数值的目标值

回归的一般方法
（1）收集数据：采用任意方法收集数据
（2）准备数据：回归需要数值型数据，标称型数据将会被转成二值型数据
（3）分析数据：绘出数据的可视化二维图将有助于对数据做出理解和分析，在采用缩减法求得新回归系数之后，可以将新拟合线绘在图上作为对比
（4）训练算法：找到回归系数
（5）测试算法：使用R2或者预测值和数据的拟合度，来分析模型的效果
（6）使用算法：使用回归，可以在给定输入的时候预测出一个数值，这是对分类方法的提升，因为这样可以预测连续型数据而不仅仅是离散的类别标签

局部加权线性回归
线性回归的一个问题是可能出现欠拟合现象，因为它求得是具有最小均方误差的无偏估计。
如果模型欠拟合将不能取得最好的预测效果，有些方法允许在估计中引入一些偏差，从而降低预测的均方误差
其中一个方法是局部加权线性回归。在改方法中，我们给待预测点附近的每一个点赋予一定的权重
局部加权线性回归使用核 来对附近的点赋予更高的权重，常用的核实高斯核

岭回归
简单来说，岭回归就是在矩阵X.T*X上加一个lambda*I从而使得矩阵非奇异，进而能对其求逆，其中I是一个单位矩阵，对角线上元素全为1，其它元素全为0
岭回归最先用来处理特征数多于样本数的情况，现在也用于在估计中加入偏差，从而得到更好的估计。
通过引入lambda来限制所有w之和，通过引入该惩罚项，能够减少不重要的参数，这项技术在统计学中也叫做缩减
缩减方法可以去掉不重要的参数，因此能更好地理解数据。此外与简单的线性回归相比，缩减法能取得更好的预测效果
与前面的方法类似，这里通过预测误差最小化得到lambda，数据获取之后，首先收取一部分数据用于测试，剩余的作为训练集用于训练参数w。训练完毕之后再测试集上预测性能
通过选取不同的lambda来重复上述测试过程，最终得到一个使预测误差最小的lambda

还有一些其他缩减方法，如lasso，LAR，PCA回归以及子集选择等。与岭回归一样，这些方法不仅可以提高预测精确率，而且可以解释回归系数

lasso 约束所有wk绝对值之和小于lambda，在lambda足够小的时候，一些系数会因此被迫缩减到0，这个特性可以帮助我们更好地理解数据。

前向逐步回归
前向逐步回归可以得到和lasso差不多的效果，但更加简单。它属于一种贪心算法，即每一步都尽可能减少误差。
一开始，所有的权重都设为1，然后每一步所作的决策是对某个权重增加或减少一个很小的值
该算法的伪代码如下：
数据标准化，使其满足0均值和单位方差
在每轮迭代过程中：
    设置当前最小误差lowestError为正无穷
    对每个特征：
        增大或缩小：
            改变一个系数得到一个新的W
            计算新的W下的误差
            如果误差Error小于当前最小误差lowestError：设置Wbest等于当前的W
        将W设置为新的Wbest

当应用缩减方法时，模型也就增加了偏差，与此同时减小了方差
