当数据拥有总舵特征并且特征之间的关系十分复杂是，构建全局模型的想法就显得太难了，也显得笨拙。
一种可行的方法时将数据集切分成很多份易建模的数据，然后利用线性回归建模
如果首次切分后仍然难以拟合线性模型就继续切分。在这种切分方式下，树结构和回归法就相当有用

CART(classification and regression trees，分类回归树)，该方法可以用于分类也可以用于回归

决策树不断将数据切分成小数据集，直到所有目标变量完全相同，或者数据不能再切分为止。决策树是一种贪心算法，它要在给定时间内做出最佳选择，但并不关心能否达到全局最优

树回归：
优点：可以对复杂和非线性的数据建模
缺点：结果不易理解
适用数据类型：数值型和标称型数据

ID3的做法是每次选取当前最佳的特征来分割数据，并按照该特征的所有可能取值来切分
如果一个特征有4种取值，那么数据将被切成4份，一旦按照某特征切分后，该特征在之后的算法执行过程中将不会再起作用。
这种切分方法被认为过于迅速
另一种方法是二元切分法，每次把数据集切成两份，如果数据的某特征值等于切分所要求的值
就进入左子树，反之则进入右子树
ID3也不能直接处理连续型特征，只有实现将连续型特征转换成离散型，才能在ID3算法中使用。
但这种转换过程会破坏连续型变量的内在性质
而是用二元切分法则易于对树构建过程进行调整以处理连续型特征。
具体处理方法是：如果特征值大于给定值就走左子树，否则走右子树
另外二元切分法也节省了树的构建时间
CART是十分著名且广泛记载的树构建算法，它使用二元切分来处理连续型变量，对CART稍作修改就可以处理回归问题

树回归的一般方法：
（1）收集数据：采用任意方法收集数据
（2）准备数据：需要数值型的数据，标称型数据应该映射成二值型数据
（3）分析数据：绘出数据的二维可视化显示结果，以字典方式生成树
（4）训练算法：大部分时间都花费在叶节点树模型的构建上
（5）测试算法：使用测试数据上的R2值来分析模型的效果
（6）使用算法：使用训练出的树做预测，预测结果可以用来做很多事

通过降低决策树复杂度来避免过拟合的过程为剪枝。

后剪枝方法需要将数据集分成测试集和训练集

用树来对数据建模，除了把叶节点简单地设定为常数值之外，还有一种方法是把叶节点设定为分段线性函数

