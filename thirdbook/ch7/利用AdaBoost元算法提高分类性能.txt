
将不同的分类器组合起来，组合结果被称为集成方法或者元算法

AdaBoost：
优点：泛化错误率低，易编码，可以应用在大部分分类器上，无参数调整
缺点：对离群点敏感
适用数据类型：数值型和标称型数据

自举汇聚法（boostrap aggregating），也称为bagging方法，是在从原始数据集选择S次后得到S个新数据集的一种技术。
新数据集和原数据集大小相等。每个数据集都是通过在原始数据集中随机选择一个样本来进行替换得到的。这里的替换就意味着可以多次地选择同一样本。
这一性质就允许数据集中可以有重复的值，而原始数据集的某些值在新集合中则不再出现。
当S个数据集建好之后，将某个学习算法分别作用于每个数据集，就得到了S个分类器，当我们要对新数据进行分类时，就可以应用这个S个分类器进行分类。

boosting是一种与bagging很类似的技术，不论是在boosting还是bagging当中，所使用的多个分类器的类型都是一致的。
但是在前者当中，不同分类器是通过串行训练而获得的，每个新分类器都根据已训练出的分类器的性能来进行训练。
Boosting是通过几种关注被已有分类器错分的那些数据来获得新的分类器

由于boosting分类的结果是基于所有分类器的加权结果而求和的，因此boosting与bagging不太一样。bagging分类器的权重是相等的，而boosting中的分类器权重并不相等，每个权重代表的是其对应分类器在上一轮迭代中的成功度。

AdaBoost的一般流程：
（1）收集数据：可以使用任意方法
（2）准备数据：依赖于所使用的弱分类器类型，本章使用的是单层决策树，这种分类器可以处理任何数据类型。当然也可以使用任意分类器作为弱分类器。作为弱分类器，简单分类器的效果更好。
（3）分析数据：可以使用任意方法
（4）训练算法：AdaBoost的大部分时间都在训练上，分类器将多次在同一数据集上训练弱分类器。
（5）测试算法：计算分类的错误率
（6）使用算法：同SVM一样，AdaBoost预测两个类别中的一个，如果想把它应用到多个类别的场合，那么就要像多类SVM中的做法一样对AdaBoost进行修改。

非均衡分类问题
其它分类性能度量指标：正确率、召回率及ROC曲线
混淆矩阵
一种针对非均衡问题调节分类器的方法，就是对分类器的训练数据进行改造
可以通过欠抽样或者过抽样来试验